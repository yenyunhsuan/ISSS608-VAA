---
title: "Hands-on_Ex04"
execute: 
  warning: false
---

## Visual Statistical Analysis with ggstatsplot

### Import package and data

```{r}
pacman::p_load(tidyverse, plotly, crosstalk, DT, ggdist, gganimate,ggstatsplot,readxl, performance, parameters, see)
```

```{r}
library(readr)
exam_data <- read_csv("data/Exam_data.csv")
```

### One sample test graph


```{r}
set.seed(1234)
gghistostats(data=exam_data,x=ENGLISH, type="bayes",
             test.values=70,xlab="English score")
```

### Two sample mean test

Compare distribution/density of female and male performance in Math test

```{r}
ggbetweenstats(
  data=exam_data,
  x=GENDER,
  y=MATHS,
  type='np',
  message=FALSE)
```

### One way ANOVA test

```{r}
ggbetweenstats(data=exam_data,
               x=RACE,
               y=ENGLISH,
               type='p',
               mean.ci=TRUE,
               pairwise.comparisons = TRUE,
               pairwise.display = 's',
               p.adjust.method = 'fdr',
               message=FALSE
               )
```

### Correlatin test

Can see Pearson correlation coefficient

```{r}
ggscatterstats(data=exam_data,
               x=MATHS,
               y=ENGLISH,
               marginal = FALSE
               )
```

### Significant Test of Association

```{r}
exam=exam_data %>% 
  mutate(MATHS_bins=
           cut(MATHS,
               breaks=c(0,60,75,85,100)))

ggbarstats(data=exam,
           x=MATHS_bins,
           y=GENDER)
```

## Toyota Corolla case with linear regression

### Import the data

```{r}
resale_car <- read_xls("data/ToyotaCorolla.xls", 
                       "data")
colnames(resale_car)
```

### Build multiple linear regression

```{r}
model <- lm(Price ~ Age_08_04 + Mfg_Year + KM + 
              Weight + Guarantee_Period, data = resale_car)
model
```

### Check multicollinearity

One way to detect multicollinearity (whether independent variables are highly correlated) is to calculate the variance inflation factor (VIF) for each independent variable.

```{r}
c <- check_collinearity(model)
plot(c)
```

### Checking normality assumption

Build model1(remove one highly correlated variable of mfg_year)

```{r}
model1 <- lm(Price ~ Age_08_04 + KM + 
              Weight + Guarantee_Period, data = resale_car)
check_n <- check_normality(model1)
plot(check_n)
```

### Check model for homogeneity of variances

Significance testing for linear regression models assumes that the model errors (or residuals) have constant variance.

```{r}
check_v <- check_heteroscedasticity(model1)
plot(check_v)
```

### Complete check

Can also check all the assumptions by one step. Influential observation is an observation in a dataset that, when removed, dramatically changes the coefficient estimates of a regression model

```{r}
check_model(model1)
```

### Parameter plot

See the coefficient direction and strength in the plot.

```{r}
plot(parameters(model1))
```

### Visualising Regression Parameters

```{r}
ggcoefstats(model1, 
            output = "plot")
```

## Visualize uncertainty of point estimates

point estimate such as mean, addressed with uncertainty like CI se: standard error measures the variability of the sample means, estimate the precision of the sample mean as an estimate of the population mean. sd/sqrt(n-1), n-1 can been thought as degree of freedom

```{r}
sum_num <- exam_data %>% 
  group_by(RACE) %>% 
  summarise(n=n(),
            mean=round(mean(MATHS),2),
            sd=round(sd(MATHS),2)) %>% 
  mutate(se=round(sd/sqrt(n-1),2))

sum_num
```

```{r}
knitr::kable(head(sum_num),format='html')
```

### Standard error visulization

```{r}
ggplot(sum_num)+
  geom_errorbar(
    aes(x=RACE,
        ymin=mean-se,
        ymax=mean+se),
    width=0.2,
    color='black',
    alpha=0.9,
    size=1)+
  geom_point(aes(x=RACE,
                 y=mean),
             stat='identity',
             color='red',
             size=2,
             alpha=1)+
  ggtitle("Standard error of mean 
          maths score by race")
```

### 95% Confidence interval

use qnorm(0.975)=1.96 to calculate lower and upper bound

```{r}
sum_num$RACE <- factor(sum_num$RACE,levels = sum_num$RACE[order(-sum_num$mean)])
ggplot(sum_num)+
  geom_errorbar(
    aes(x=RACE,
        ymin=mean-1.96*se,
        ymax=mean+1.96*se),
    width=0.2,
    color='black',
    alpha=0.95,
    size=1)+
  geom_point(aes(x=RACE,
                 y=mean),
             stat='identity',
             color='red',
             size=2,
             alpha=1)+
  ggtitle("95% confidence interval of mean maths score by race")
```

### Uncertainty of point estimates with interactive error bars

```{r}
data <- highlight_key(sum_num)
p <- ggplot(data)+
  geom_errorbar(
    aes(x=RACE,
        ymin=mean-2.32*se,
        ymax=mean+2.32*se),
    width=0.2,
    color='black',
    alpha=0.99,
    size=1)+
  geom_point(aes(x=RACE,
                 y=mean),
             stat='identity',
             color='red',
             size=2,
             alpha=1)+
  ggtitle("99% confidence interval of mean maths score by race")

gg <- highlight(ggplotly(p),"plotly_selected")

crosstalk::bscols(gg,DT::datatable(data),widths = 5)
```

### Confidence interval plot with ggdist

```{r}
exam_data %>% 
  ggplot(aes(x=RACE,y=MATHS,))+
  stat_pointinterval()+
  labs(
    title="Visualising confidence intervals of mean math score",
    subtitle = "Mean Point + Confidence-interval plot")
  
```

### Use stat_gradientinterval

```{r}
exam_data %>% 
  ggplot(aes(x = RACE, 
             y = MATHS)) +
  stat_gradientinterval(
    fill='skyblue',
    show.legend=TRUE
  )+
  labs(
    title = "Visualising confidence intervals of mean math score",
    subtitle = "Gradient + interval plot"
  )
```

### Hypothetical Outcome Plots

```{r}
devtools::install_github("wilkelab/ungeviz")
```


transition_states means create sequence of frames to have animation of changes

